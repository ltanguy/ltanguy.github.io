{"categories":[{"title":"cloud","uri":"https://ltanguy.github.io/categories/cloud/"},{"title":"security","uri":"https://ltanguy.github.io/categories/security/"}],"posts":[{"content":"After hearing the news that Zoom was moving part of its infrastructure to Oracle Cloud and reading a few articles on that matter, I was curious to see with my own eyes what Oracle\u0026rsquo;s offer looked like.\nPlease note that I\u0026rsquo;m familiar with (but not affiliated to) AWS, much less with Azure or GCP, so this article is in no way exhaustive nor a comprehensive benchmark of the cloud market. I may be a little biased, but I\u0026rsquo;ll try to keep it to a minimum.\nFirst impressions At first sight, the UI of the console feels a lot cleaner and less cluttered than the AWS console. The interfaces of all services (even those dedicated to Oracle RDBMS !) also follow the same UX principles, which is nice. On AWS, each service team is also responsible of the UI displayed on the console, which may help them deliver more rapidly, but comes at a cost in terms of homogeneity of the UX.\nThe geographic presence of Oracle Cloud is less dense than AWS, as it was foreseeable given how Oracle was late on the market. At the time of this writing, Oracle has 17 regions, but only 4 have 3 Availability Domains (equivalent of Availability Zones in AWS jargon), 2 in Europe, 2 in the US. This numbers should be compared with the 24 regions of AWS, almost all with 3 Availability Zones. This may not be perfect for every countries on earth, but at least in should allow US and european customers of Oracle to have a nominal/backup topology without leaving their home continent.\nIAM IAM and user management is usually the first step when building a cloud infrastructure. Oracle follows a slightly different approach than AWS by introducing a notion of compartment, which has the same objectives as Organizational Units and Accounts in an typical enterprise AWS setup with AWS Organizations.\nCompartments are a logical object where you attach the objects you create in Oracle Cloud: Users, Groups and all compute/storage/whatever resources, but also IAM policies. Compartments can also have sub-compartments which inherit policies from their parents.\nOnce your account is created you end up with your first compartment, the root compartment and an admin account and group. You can then create child compartments for each perimeter of your organization.\nI must say that this feels a lot simpler and more integrated than the tree of accounts and OU in AWS Organizations. Admittedly, this is probably due to the fact that the first IAM releases in AWS were not designed to segregate resources between different user populations. AWS chose afterwards not to break the backward compatibility by adding a layer on top of this notion of accounts. One clear benefit of the Oracle approach is that cross-account policies feel a lot more natural to write and you don\u0026rsquo;t end up with a dozen of administrative accounts to secure - that should not be used in day-to-day operations btw.\nOne interesting difference lies also in the way policies are defined: Oracle leveraged a simili-natural language approach for defining policies, which make them a lot more readable than their AWS counterpart.\nAs an example: Allow group admins to manage database-family in compartment dev [where advanced conditions]\nSome important notes:\n You can only allow something in policies. Everything that is not allowed is denied. This approach feels sound from a security perspective, but I fear it could lead in some edge cases to defining a lot of policies. You can only define policies on groups, not individual users. Again, this also feels like a good idea by forcing administrators to define clear responsabilities for their users. Verbes are limited to 4 values: inspect, read, use, manage. The verb defines the set of APIs in the referenced resource that the user can use, instead of explicitly listing which API are allowed. Resources can be individual resources or resources families (like database-family in the example) Some advanced conditions can be leveraged by relying on properties of the policy itself (by example, to restrict which API in the resource-family is authorized) or properties of the target of the policy (like the group name).  Networking The concepts are very similar to AWS and any AWS user should quickly find their marks once the specific vocabulary is assimiled (VCN vs VPC, etc.).\nThe only major difference that struck me is how filtering is handled in Oracle Cloud. Security Lists are at first glance an equivalent of Network ACLs in the AWS world. They are attached to a subnet but the filtering occurs on the Virtual NIC attached to an instance. Security Lists doesn\u0026rsquo;t actually control the traffic entering or leaving the subnet like Network ACL, but rather the traffic entering or leaving a Virtual NIC: subtle but disturbing as those lists do filter the traffic between instances in the same subnet.\nOracle Cloud also have the notion of Network Security Groups. This feature was apparently added afterwards and doesn\u0026rsquo;t seem to be pushed that much as the gold standard for traffic filtering. A lot of content (documentation, videos, etc.) still references Security Lists as the way to go to filter traffic. This is bad and the documentation should be fixed to rely instead on Network Security Groups for most of the filtering effort.\nThe most important problem with Security Lists (which are also true for Network ACLs in AWS) is that the filtering rules create a coupling between the applications and the network topology: changing an IP on an instance, adding a new instance, removing an instance, etc. all have potentially an impact on one or more Security Lists. This also leads, as in traditional firewall architectures, to a decay in the quality of filtering and it doesn\u0026rsquo;t scale well: instances are destroyed, but not the rules, or the rules are too wide\u0026hellip;\nFor large deployments, the lack of an equivalent service to the AWS Transit Gateway may make the maintainance of the different Route Tables in each subnet very complex.\nCompute and Storage Oracle claims that the entire compute architecture is built on the same Bare Metal stack. VM instances are launched on a Bare Metal instance equipped with an hypervisor. Even if it\u0026rsquo;s intellectually satisfying, I see the benefit here for Oracle (less hardware types to manage), but not really for the customer. VMs are enough for the vast majority of use cases, unless you have very I/O-intensive applications (let\u0026rsquo;s say\u0026hellip; a highly sollicitated database). Most people really don\u0026rsquo;t care that Bare Metal is a first-class citizen.\nOracle uses Xen as the hypervisor for their VM infrastructure and thus is very similar to the previous generation of EC2 instances on AWS.\nSince then, AWS moved to an architecture called Nitro (more info on it in this video), which introduced a lightweight hypervisor with strong security guarantees (no human operator intervention possible for example) and a lot of things offloaded in hardware to ad-hoc chips (notably all encryption on block storage). On the technology side of things, AWS seems to have a clear lead.\nStill, this doesn\u0026rsquo;t necessarily mean that AWS have a better compute/price ratio in practice, but at least this indicates they could.\nOn the storage side, Oracle leverages exclusively SSD disks and provides an SLA on I/Os. Again, I think most companies don\u0026rsquo;t really need this kind of SLA and quite frankly I\u0026rsquo;m not sure I would be willing entering a dispute with the legal department of Oracle if those SLAs were not met :-)\nPricing Pricing seems relatively similar between AWS and Oracle, except for one point: outbound data pricing.\nAWS is notorious for its convoluted and expensive pricing for data transfer. It is for almost any client one of the more costly spot in their bill. The problem is summarized in this figure:\n Source: Duckbill Group\n  Oracle outbound data transfer pricing is simple:\n Outbound traffic costs $0.0085/GB/month (at the time of this writing) after the first 10TB (billing starts at 1GB in AWS\u0026hellip;) Everything else is free, including FastConnect data transfers (equivalent of DirectConnect in AWS)  It\u0026rsquo;s basically at least 10 times cheaper than AWS and a lot more predictable. It\u0026rsquo;s probably the main reason why Zoom signed with Oracle.\nThis is a very smart move from Oracle, for at least 2 major reasons:\n If you want to compete with AWS, you have to attack where they are weak or too complex. This very interesting article by Corey Quinn elaborates on this subject if you\u0026rsquo;re curious. It makes a lot of sense for a company who also specializes in data management  To elaborate on this second point, if you\u0026rsquo;re thinking about building an hybrid cloud setup and host your datawarehouse or your entire data platform in a public cloud to benefit from the higher level of automation than the one you probably have (or not) on premises, this is one major pain-point that goes away instantly. Once your data are in Oracle Cloud, the natural next step is also to host some applications on it. This is clever.\nDatabases This makes a good transition with this section.\nThe RDBMS database offering is a bit complex. For hosting Oracle databases, you have 3 solutions:\n Host a database on a VM or Bare Metal instance. This is more or less identical to a classic private cloud Oracle service offer: the installation, patching, backup and maintenance tooling is plugged to the console and most operations can be not done through it. But\u0026hellip; you still have a shell access to the server, you can modify, fine-tune or break the default configuration. I\u0026rsquo;m not a big fan of this unless you have really valid reasons to do so (and you often really don\u0026rsquo;t). Buy a partition on an Exadata system: More or less the same, but with a higher level of automation. Use the new Autonomous Database, which is basically a fully managed Oracle Database on an Exadata (which is abstracted away from the user).  This third option aims to compete with Relational Database Service (RDS) from AWS. It\u0026rsquo;s interesting if you\u0026rsquo;re an Oracle shop and develops mainly in Java or rely on Oracle middleware, less so if you\u0026rsquo;re developing in brand new languages like PHP, JS on Node, Go, Ruby, Python, etc. Despite those languages being able to connect to an Oracle database, the drivers are a lot less supported, the community support is reduced and the APIs are generally\u0026hellip; meh.\nAutonomous Database are also surprisingly lacking some important features found in RDS, for example the ability to create replicas in another regions, which are useful in Disaster Recovery scenarios. You\u0026rsquo;re basically stuck to restore a backup on another instances in another region, with a guaranteed data loss.\nOracle also proposes a MySQL service similar to the first bullet point before. Common procedures are automated, but you\u0026rsquo;ll have to manage or break the database yourself.\nOn the NoSQL front, Oracle doesn\u0026rsquo;t provide the same breadth of choices than AWS (DocumentDB, Neptune, Redis, DynamoDB, Elasticsearch, etc.), only Oracle NoSQL which is not that well known nor covers some of those use cases.\nFree Tier This may seem like a trivial matter, but Oracle is the only major cloud provider to offer a permanent and usable free tier:\n 2 VMs with 1/8 CPU and 1GB RAM 2 autonomous databases instances with 1 CPU and 20GB of storage 2 block volumes for a total of 100GB, 10GB or Object Storage and 10GB of Archive Storage 10TB/month of outbound data transfer and a load balancer with 10Mbps of bandwith  This is in my opinion usable for light hacking and a lot better than what AWS offers (mainly Lambda and DynamoDB which strangely are also services that have a propensity to lock you in with AWS).\nOther services Oracle offering is probably competitive on the main services you expect from a cloud vendor, but lacks a lot of the bells and whistles you\u0026rsquo;ll find on AWS.\nI tried to list the important services in AWS I feel are lacking in Oracle Cloud:\n Config: very important to prevent mistakes or detect bad practices when deploying cloud resources. It would not be that problematic if popular alternatives like Cloud Custodian supported Oracle Cloud. Lambda: even if you don\u0026rsquo;t develop serverless applications, it\u0026rsquo;s also an effective way to host scripts to automate whatever you want on your infrastructure. Systems Manager: great to manage your instances without enabling SSH everywhere and benefiting from strong authentication mechanisms (MFA) CloudFront and Shield: to distribute content and protect hosted resources GuardDuty: to detect abnormal behaviours in your infrastructure Secrets Manager and Certificate Manager: great for managing and automating the management of secrets in applications. Hashicorp Vault is a great alternative, but you\u0026rsquo;ll need to operate it yourself on Oracle Cloud. Organizations: despite being complex, it also automates a lot of things when provisioning users in your accounts and allows fine grained control of rights through Service Control Policies (SCP)  Conclusion: Who is it for ? If you\u0026rsquo;re Zoom and generates petabytes a month of outbound data, Oracle Cloud is almost a no-brainer given the huge difference in pricing compared to their competitors. Does it mean you host all your infrastructure on it ? Not so sure.\nFor existing customers of Oracle products (especially the RDBMS and products from the middleware offering), it can make a lot of sense to host them on Oracle Cloud in an hybrid cloud approach. The offering is clearly not ridiculous compared to AWS on the basic services that you can expect from a public cloud provider and those companies can level up the automation level on these apps (which are often hard to maintain on premises).\nIt can also appeal to companies which follow a strict \u0026ldquo;cloud-agnostic\u0026rdquo; or \u0026ldquo;multi-cloud\u0026rdquo; approach to public cloud (despite being in my opinion a not so good idea in practice\u0026hellip; but that\u0026rsquo;s a topic for another day), where only basic compute and storage services are leveraged.\nIf you\u0026rsquo;re a small structure and want the provider to manage almost everything while having few constraints on your technology stack, Oracle will probably not be your first choice, unless you largely underestimate the effort and costs needed to operate your own infrastructure :-)\nFinally, I must say that I was pleasantly surprised by Oracle Cloud. I had low expectations but they did deliver almost all the basics and the offering seems solid (with a few gotchas, especially on the database space). Keep in mind though that I only scratched the surface and that the little features of each services that are not marketed can make the difference between an easy setup and a nightmare. It takes time and a lot of customer usage to get there.\n","id":0,"section":"posts","summary":"After hearing the news that Zoom was moving part of its infrastructure to Oracle Cloud and reading a few articles on that matter, I was curious to see with my own eyes what Oracle\u0026rsquo;s offer looked like.\nPlease note that I\u0026rsquo;m familiar with (but not affiliated to) AWS, much less with Azure or GCP, so this article is in no way exhaustive nor a comprehensive benchmark of the cloud market. I may be a little biased, but I\u0026rsquo;ll try to keep it to a minimum.","tags":["oracle","cloud","aws"],"title":"Oracle Cloud Impressions","uri":"https://ltanguy.github.io/2020/05/oracle-cloud-impressions/","year":"2020"},{"content":"TLS Mutual Authentication (TLS MA or more commonly nowadays mTLS) is increasingly being leveraged for securing links between applications (see the section of Istio\u0026rsquo;s documentation on security as an example).\nIt provides a much more robust and secure solution than static shared credentials:\n Shared credentials are susceptible to brute-force attacks. Shared credentials should be periodically rotated. In practice, this is almost never the case as the process is particularly painful and implies the rotation of the credentials at the server and client level simultaneously. In constrast, mTLS allows the client or the server to rotate their public/private key pair independently. When done correctly, the client private key never leaves the client infrastructure and is thus harder to steal.  That said, mTLS is admittedly harder to setup and has some pre-requisites: having deployed a Certificate Authority (CA), having a good operation tooling for the deployment and maintenance of those secrets and more importantly having devs and ops understand how the pieces fits together.\nI was involved recently in a design discussion on how to secure an edge gateway facing multiple third-parties, relying on mTLS to authenticate connections from remote services. I was painfully reminded how hard it is, even for seasoned technical or security specialists, to reason about mTLS and what it means in terms of authorization for applications. For most people, mTLS is a very different beast than all other authentication methods. Spoiler: it\u0026rsquo;s not (that much).\nSome important properties of certificates in a Public Key Infrastructure X.509 Certificates are basically a bunch of metadatas, notably:\n The public RSA/DSA/EC public key associated with the private key the service holds A period of validity (NotBefore and NotAfter) The identity and a signature of the CA that issued the certificate Informations on how to use this certificate (Constraints) A Subject attribute  This Subject attribute has the form of a X.500 Distinguished Name (for example: CN=HR System,OU=HR,O=ACME). The only guarantee CA provides is that all certificates with a given Subject attribute are emitted to the same person or application.\nQuoting RFC 5280 on X.509 certificates:\n Where it is non-empty, the subject field MUST contain an X.500 distinguished name (DN). The DN MUST be unique for each subject entity certified by the one CA as defined by the issuer field. A CA MAY issue more than one certificate with the same DN to the same subject entity.\n This happens usually when:\n A renewal of the certificate happened. Usually the previous certificate is expired or revoked. The application that represents this Subject is deployed in HA. Each service has its own private key and certificate.  All other attributes in the certificate (notably subjectAltName) have usually no unicity guarantees.\nAlso, the DN of the Subject attribute is arbitrary. It can be as simple as CN=HR System or as complex as CN=HR System,OU=Global HR,O=ACME,C=FR. All certificate authorities have different policies for naming.\nIt\u0026rsquo;s important to be aware that in no way the CA guarantees that the CN attribute in the DN is unique. Two certificates emitted by the CA to two different parties can share the same CN, for example CN=HR System,OU=Global HR,O=ACME,C=FR and CN=HR System,OU=Other HR Service,O=ACME,C=FR.\nmTLS Basics I won\u0026rsquo;t dive here into the details of mTLS. For a refresh on mTLS, see this article for example.\nIn summary, mTLS implies two things:\n the server exposes a non-expired server certificate, signed by a CA that the client trusts, that references the FQDN of the server in the subjectAltName attribute (not the CN of the certificate) the client has a non-expired client certificate, signed by a CA the server trusts for authentication.  That\u0026rsquo;s basically it. It should be noted here that most of the configuration is done at the level of the middleware hosting the code (an Application Server or Web Server). The code is generally unaware of how the TLS configuration is done.\nmTLS with a single trusted CA Let\u0026rsquo;s imagine a scenario where a service exposes two APIs, A and B. API A can be accessed by client 1 and API B can only be accessed by client 2. We absolutely do not want client 1 to access API B and inversely. Both clients use certificates emitted by the same Certificate Authority.\n This is the easy road. In this scenario, the Subject DN of the client certificate is the identity the service can rely on for authorization on its API.\nIt\u0026rsquo;s important though to rely on the whole DN and not only the CN as it may not be unique at the whole CA level, as seen previously. It particularly holds true if the client uses a certificate emitted by a public CA.\nmTLS with multiple trusted CA This is there where things usually go wrong.\nLet\u0026rsquo;s take the previous scenario, but now each client uses a client certificate emitted by two different certificate authorities. This is a frequent setup if you expose your services to partners through an edge gateway.\n Depending on the technology used for implementing the service, you either:\n trust globally all the CAs on all your services. This is the case for Java, where the trust is global at the JVM level. create something similar to Virtual Hosts in Apache or Nginx, where you trust only one CA and do authorization in this compartment.  Global trust In the first case, if you rely solely on the DN of the Subject to authenticate the clients, it is possible for an attacker to generate a certificate on the second CA with the same DN and access your service. This is bad.\nThis is not uncommon. Let\u0026rsquo;s look at the documentation of authorizations in Apache Kafka:\n By default, the name of the principal identified by a TLS/SSL certificate is the DN (X.500 Distinguished Name) of that certificate (also known as the Subject), which uses the form CN=writeuser,OU=Unknown,O=Unknown,L=Unknown,ST=Unknown,C=Unknown. You can use ssl.principal.mapping.rules to translate the DN to a more manageable principal name.\n If your Kafka cluster is trusting more than one CA, you\u0026rsquo;re potentially vulnerable. In this context, the usage of mapping rules to extract an identifier from the DN adds another layer of danger.\nIn general, you can be tempted to check the format of the DN or trusting too much the issuance process of the CA. For example: thinking that the issuance process of the CA ensures that you\u0026rsquo;re necessarily authorized by ACME corporation to request certificates with a subject containing O=ACME.\nChances are that it is not the case.\nVirtual Hosts In the second case, you\u0026rsquo;re at first sight back to the single CA scenario, but this strategy will be probably hard to maintain as you add other CAs.\nThis strategy will also have impacts on how you expose your services to partners, as the server needs to determine on which Virtual Host it needs to route the incoming request:\n Use a different FQDN for each Virtual Host (if all your clients supports Server Name Indication / SNI). Use a different port for each Virtual Host Use a different IP for each Virtual Host (and as a consequence a different FQDN)  This also couples a lot of your applicative security to the infrastructure configuration, which is never a good idea.\nA simple universal solution People familiar with Radius or Kerberos will probably remember fondly the notion of realms.\nmTLS can (and must) be considered similarly:\n Trusting a CA on your service basically means trusting the realm that is managed by this CA. This realm is defined by the Subject DN of this CA, which will appear in each certificate emitted by it (Issuer attribute). It may still be a good idea to ensure that you\u0026rsquo;re not trusting two CAs with the same Subject (though it\u0026rsquo;s unlikely). The client identity is determined by the combination of the Subject DN and the Issuer DN of its certificate, for example CN=HR System,OU=HR,O=ACME,C=FR@O=ACME,C=FR. This identifier is guaranteed to be unique across all certificates emitted by the CAs you trust.  This approach is simple, works independently of the number of CAs you trust in your application and makes no hypothesis on the format of the Subject DN or Issuer DN. It still allows for independent renewal of the client certificate. By following this principle, trusting globally all CAs also have a low security impact and allows us to simplify the configuration of the middleware.\nIf the application in the examples is an edge gateway and fine-grained authorization is done in backend-services, it\u0026rsquo;s equally important to propagate those two attributes.\nThe inherent complexity of mTLS makes us easily forget the basic properties of authentication and authorization: the user must prove its identity, its identifier must be unique at the system level and authorization must be based on this unique identifier.\nHope this post is helpful. Comments ? Hit me on Twitter.\n","id":1,"section":"posts","summary":"TLS Mutual Authentication (TLS MA or more commonly nowadays mTLS) is increasingly being leveraged for securing links between applications (see the section of Istio\u0026rsquo;s documentation on security as an example).\nIt provides a much more robust and secure solution than static shared credentials:\n Shared credentials are susceptible to brute-force attacks. Shared credentials should be periodically rotated. In practice, this is almost never the case as the process is particularly painful and implies the rotation of the credentials at the server and client level simultaneously.","tags":["mTLS","authorization","authentication"],"title":"Authorization with TLS Mutual Authentication","uri":"https://ltanguy.github.io/2020/05/tls-ma-authorization/","year":"2020"}],"tags":[{"title":"authentication","uri":"https://ltanguy.github.io/tags/authentication/"},{"title":"authorization","uri":"https://ltanguy.github.io/tags/authorization/"},{"title":"aws","uri":"https://ltanguy.github.io/tags/aws/"},{"title":"cloud","uri":"https://ltanguy.github.io/tags/cloud/"},{"title":"mTLS","uri":"https://ltanguy.github.io/tags/mtls/"},{"title":"oracle","uri":"https://ltanguy.github.io/tags/oracle/"}]}